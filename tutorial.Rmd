---
title: "Working with Climate Data"
author: "Nassos Stylianou & Becky Dale"
date: "18/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


NOTE - CHANGE THIS TO BE A LITTLE MORE GENERAL ABOUT GRIDDED DATA, RATHER THAN JUST NETCDF ONLY

## What are netCDF files?

NetCDF is a widely used format for exchanging or distributing climate data. These types of files were originally developed for storing and distributing climate data, such as those generated by climate simulation or reanalysis models, but the format and protocols can and have been used for other 'gridded' data sets or where multidimensional arrays of data are generated. 

NetCDF files contain metadata that describes what is contained in a file, such as the latitude and longitude layout of the grid, the names and units of variables in the data set, and “attributes” that describe things like missing value codes, or offsets and scale factors that may have been used to compress the data. 


The first thing to get your head round before diving in is the structure of a netcdf file. They are multi-dimensional arrays, which often hold data in numerous 'bands', these tend to be related to time, for example there could be one band for each month or year, and each band tends to have three dimensions, which tend to be latitude, longitude and the value (temperature, rainfall, number of days over X degrees or whatever else).

## How can you work with netcdf data?

There are a number of different ways, software and packages to work with netCDF files. A lot of analysis and data wrangling with these files is done with Python, however they can be read into R and there are a few packages for working with them. You can also load them up into QGIS - they are pretty much raster files really. Software like Panoply, developed by NASA, is also a great to get an initial feel of the data. Each option has different advantages and disadvantages. In the sections below I will go through how to work with each one and at the bottom of each one will include a user case where this option makes sense, in my experience so far (there could be many other use cases I haven't got to).


## Viewing your data in Panoply

PANOPLY SCREENSHOTS AND NOTES

## Working with your data in R

INTRO

### Load the packages you need

```{r load_libs, waring = FALSE, message = FALSE}
library(ncdf4) # package for netcdf manipulation
library(raster) # package for raster manipulation
library(RNetCDF) # package for working with netcdfs
library(rgdal) # package for geospatial analysis
library(chron) #package to help with time conversions
library(dplyr) #package for data manipulation in R
library(lubridate) #package  for working with dates in R

```


### Reading a gridded file into R (nc_open from ncd4)

You can open a netcdf file using the `nc_open()` function from the `ncdf4` library. 

At its simplest, all you need is the filepath of your netcdf file, and the data from the file into a new variable you can access in R. 

The file we are using is inside our `data` folder. So all you need to do to load it in is run the following code:

``` {r load_data}

# This loads the data in and saves it to a variable called nc_data
nc_data <- nc_open('~/Dropbox (BBC)/Visual Journalism/Data/2022/working_with_climate_data_nicar22/data/cru_ts4.05.2011.2020.tmp.dat.nc')

# To see some basic information about the data, print out the file. 
print(nc_data)

```

When you print the file, you will see some basic information about the file, which resembles the metadata of the file. It does not print the data in the file as a standard dataframe would for example. You can also see that the file is essentially a list. 

So the informaiton below is key to how we can go about re-shaping the data from its list/raster format to a more 'rectangular', dataframe-like format, as we will construct the dataframe from the dimensions below. 

### Extracting variables 

As you can see from the metadata, there are the dimensions longitude (lon), latitude (lat), time, which we want to examine first. 

To extract a variable from the netcdf file, we can use the `ncvar_get()` function from the `ncdf4` package. The first argument you should pass to the function is the name of your netcdf file in R and the second argument is the name of the variable (dimension in this case) we want to extract. The `ncvar_get()` function actually extracts the data as arrays, so we will need to do some formatting to them later on to change the data type. 

We know that the longitude is stored in the variable/dimension called "lon" as we can see that in the metadata above, under the dimensions. We can also find the names of the different dimensions and variables in R, if we run `View(nc_data)`. Running the `View()` function should open up a new tab in R, which shows us all the different list elements of the netcdf file. You can click on the `dim` and `var` lists to see the names of the dimensions and variables.

#### Longitude 

Let's start with the longitude. 

``` {r lon_variable, message = FALSE, warning = FALSE}

# The line of code below extracts the longitude variable from the netcdf file as an array into the lon variable.
lon <- ncvar_get(nc_data,"lon")

# Using the dim() function and passing our new lon variable to it gives us the dimensions of the l and saves it to the nlon variable.
nlon <- dim(lon)

# These functions just give you some insight into the lon variable
head(lon)
tail(lon)
max(lon)
min(lon)

```

#### Latitude 

We then need to do the same with the latitude, to have both the longitude and latitude in different variables. 


``` {r lat_variable, message = FALSE, warning = FALSE}

# The line of code below extracts the latitude variable from the netcdf file as an array into the lat variable.
lat <- ncvar_get(nc_data,"lat")
# Using the dim() function and passing our new lat variable to it gives us the dimensions of the lat and saves it to the nlat variable.
nlat <- dim(lat)
# These functions just give you some insight into the lat variable
head(lat)
tail(lat)
max(lat)
min(lat)

```


Running the different `head`, `tail`, `max` and `min` functions will show you the first few and last few latitude and longitude values for your data, which are in essence the centre points of your grid in the raster dataset. 

You can also see that `the ncvar_get()` function actually extracts the data as 1D arrays. 

#### Time 

The next step is to extract the time variable from your dataset - this is a little trickier because the time variable will need to be interpreted based on the time units, it is rarely a straightforward 'date'. 

To find out what time unit you are dealing with for the specific dataset in question, a lot of the time this should be in the metadata information that you get when you `print()` the netcdf dataset after you have loaded it in. So if you scroll back up to look at the information printed out for this dataset, you can see the `units` for the `time` dimension are:  


``` {r time_variable, message = FALSE, warning = FALSE}

time <- ncvar_get(nc_data,"time")

time

```

So by printing out the time variable after you have extracted it from the netcdf file, each time unit is a number rather than a date and as we know from the information we have, that number is `days since 1900-1-1`.

This will need to be factored in to convert the formats a little later on. 

To actually turn the units from the time dimension into a variable, we use the ncatt_get() function to extract the units attribute

``` {r time_variable_units, message = FALSE, warning = FALSE}

tunits <- ncatt_get(nc_data,"time","units")

# And just like the lat and lon, we can get the time variable dimensions as well using the dim() function.
ntime <- dim(time)

```

#### Data variable - temperature

We can extract the data from the array in a very similar way. We know the data variable we are after (temperature) is called `tmp`, so we can use the same `ncvar_get()` function:

``` {r tmp_variable, message = FALSE, warning = FALSE}

tmp_array <- ncvar_get(nc_data, "tmp")

```

So if we look at all our global environment variables now, we have the longitude, latitude, time, and temperature arrays. You can now get an understanding of the dimensions of each of them and how they will fit in together. The temperature array is a 3D array and matches the dimensions of the lon, lat and time ones, which are 1D arrays. 

So essentially, the dimensions of the array are 720 lons, 360 lats and 120 times (10 years, 12 months for each year.

You can verify this by running dim(tmp_array) and the dims of each variable.

The other really useful thing to find out about your temperature data variable at this stage is what potential fill value was used for missing data. This is a common feature of some netcdf files which may be different to standard dataframes in R. You can find the fill value by running the `ncatt_get()` function as below - the name of your data as the first argument, the name of your variable as the second argument and "_FillValue" as the third argument

``` {r fill_value, message = FALSE, warning = FALSE}

# Finds the fill value used for missing data for the precipitation variable
fillvalue <- ncatt_get(nc_data, "tmp", "_FillValue")
fillvalue
```

The fill value in this particular case is `9969209968386869046778552952102584320`.


The best thing to do is probably first, might just be for housekeeping, replace any fill values with 'NA's, as would be standard practice in R. Here's how:

``` {r fill_value_assign, message = FALSE, warning = FALSE}
# Finds the fill value used for missing data for the temperature variable
tmp_array[tmp_array == fillvalue$value] <- NA

```


### Reshaping your data

Netcdf files and the data within them are naturally raster 'slabs' (a longitude by latitude “slice”), bricks (a longitude by latitude by time), or 4-d arrays (e.g. a longitude by latitude by height by time), while most data analysis routines in R expect 2-d variable-by-observation data frames.

In addition, as we have seen with the time variable already, time is usually stored in the “time since” format that not a proper date format

### Converting the time variable 

So before reshaping the data format to a 2D dataframe format we can work with, we will need to convert the time variable.

The time variable in “time-since” units can be converted into actual time values by splitting the time tunits$value string into its component parts, and then using the chron() function to determine the absolute value of each time value from the time origin.

``` {r reshaping_Time, message = FALSE, warning = FALSE}
# example of how we can convert time -- split the time units string into fields
tustr <- strsplit(tunits$value, " ")
tdstr <- strsplit(unlist(tustr)[3], "-")
tmonth <- as.integer(unlist(tdstr)[2])
tday <- as.integer(unlist(tdstr)[3])
tyear <- as.integer(unlist(tdstr)[1])

time_cols <- chron(time, origin=c(tmonth, tday, tyear)) %>%
  lubridate::mdy() %>%
  as.character()

```

So if you look at the time values in days since, this is what you get:

``` {r original_time, message = FALSE, warning = FALSE}
print(time)

```


If we print out the converted time, this is what we get:

``` {r converted_time, message = FALSE, warning = FALSE}

print(time_cols)

```

So you can see that the time-stampsfor the particular set of data is The “time-stamp” for this particular data set is the mid-point of the interval for each month of the year for 2011 to 2020. There are other ways in which the “time” associated with a long-term mean is represented in climate stats, but essentially the code above shows us how to change the date format from days since to an actual date - whatever that date ends up representing.

A note on time and formatting - bear in mind there are a number of different ways that time data will be represented and you can turn it into more meaningful date formats in different ways once you understand that date pattern. You do not always have to change the dates in the way it was done above by unlisting and stringspliting, so always be ready to adapt. 

### Turning the data into a dataframe

The first step in the process is to convert the array into a vector - the code and process below will only work only if the netCDF file - and by default the data array - follows the “CF” conventions, i.e. that the variable has been defined to have dimensions nlon by nlat by nt, in that order. We have seen from interrogating our data above, that this is the case for our data - and most netcdf data you will encounter should follow the same format.

So if you remember from earlier, we had turned our data into a really large 'temperature' array by identifying the variable where the precipitation data was held and using the ncvar_get() function. 

This is what we had done: `tmp_array <- ncvar_get(nc_data,"tmp")`

So once we have our array, what we then need to do is turn it into a vector

``` {r vector_conversion, message = FALSE, warning = FALSE}

tmp_vector_long <- as.vector(tmp_array)

```

To check just how long this vector is, run length(tmp_vector_long) - and you can see that the length is 31104000

So we know that our data is made up of latitude, longitude, time and temperature variables. We know that we have 120 time values (12 monthls over 10 year) and the number of rows will be the number of longitude values by the number of latitude values, as these are grids.

So we then reshape the vector into a matrix based on this logic and using the nlon, nlat and ntime values we created earlier when evaluating our netcdf dataset.


``` {r matrix_conversion, message = FALSE, warning = FALSE}
# reshape the vector into a matrix
tmp_matrix <- matrix(tmp_vector_long, nrow=nlon*nlat, ncol=ntime)

dim(tmp_matrix)

```

So in total this should give us a matrix sized 259200 (720 lons x 360 lats) by 120 (time), which adds up to 31104000 from earlier Bear in mind that we are not just multiplying these two values (720 by 360) to get our number of rows when creating the matrix, as the number of rows will depend on the number of latitude and longitude sized grids, so we use the variables nlon and nlat we created that are specific to the dimensions of this specific dataset.

Then reshape that vector into a 259200 by 120 matrix using the matrix() function, and verify its dimensions, which should be 259200 by 120.

S0 lets see what our matrix actually looks like - lets chekc it without the NAs, as a lot of the NAs would be for values in the sea, of which there are many.

``` {r print_matric, message = FALSE, warning = FALSE}

head(na.omit(tmp_matrix))

```

So this matrix is basically the temperature data only, so we now need to add the latitude and longitude values to this, which we can do by creating a second dataframe and binding the two together.


``` {r lon_lat_matrix, message = FALSE, warning = FALSE}
# creating lon lat matrix
lonlat_matrix <- as.matrix(expand.grid(lon,lat))

# bind the two matrices together and turn into a dataframe
tmp_dataframe <- data.frame(cbind(lonlat_matrix, tmp_matrix))

```

Essentially, as you can see the dataset starts in the far corners of the world in a WGS84 system, hence the NAs as there are no values for the ocean in our dataset.

The other thing that you can see is that there are no column names - so we can set these ourselves. We know the first two columns are the longitude and latitude, as when we joined up the lonlat matric with the tmp matrix, we put the lonlat matrix first. Then, the column names for each of the tmp columns are the time columns. This is where our time conversion comes into use, as we can use that to set our column names.

``` {r column_names, message = FALSE, warning = FALSE}
lon_lat_cols <- c("lon", "lat")

tmp_cols <- c(lon_lat_cols, 
              time_cols)

colnames(tmp_dataframe) <- tmp_cols


```

NEXT STEPS

PLOT?

### Using the raster package to load in data and do analysis (anomaly analysis in fewer steps)

